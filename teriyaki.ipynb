{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Apple Chicken:\n",
      "\n",
      "1. Preheat oven to 375 degrees.\n",
      "2. Coat chicken in flour.\n",
      "3. Place chicken in baking dish.\n",
      "4. Cut up apples into thin slices and place on top of chicken.\n",
      "5. Sprinkle with salt.\n",
      "6. Bake for 30-\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"text-davinci-002\",\n",
    "                                        prompt =  texts,\n",
    "                                        temperature = 0.6,\n",
    "                                        top_p = 1,\n",
    "                                        max_tokens = 64,\n",
    "                                        frequency_penalty = 0,\n",
    "                                        presence_penalty = 0)\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "recipe = 'Provide a cooking recipe based on the following ingredients: \\\\n \\nApple \\\n",
    "          \\n \\nFlour \\\n",
    "          \\n \\nChicken \\\n",
    "          \\n \\nSalt'\n",
    "\n",
    "GPT_Completion(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conditional problem files to /home/alessio/planning/teriyaki/generator/problems/conditional...\n",
      "Done.\n",
      "All operations completed. Happy planning!\n",
      "24 duplicates deleted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import filecmp\n",
    "\n",
    "## cleaning work directories \n",
    "!rm dataset/problems/\\* &> /dev/null\n",
    "!rm dataset/plans/\\* &> /dev/null\n",
    "!rm dataset/plans_ipc/\\* &> /dev/null\n",
    "!rm dataset/formatted/\\* &> /dev/null\n",
    "\n",
    "## 8000 data points should improve performance between 2x and 4x\n",
    "%run generator/pddl_confgen.py 4 8000 -r 24 -co\n",
    "!mv -v generator/problems/conditional/* dataset/problems > /dev/null\n",
    "\n",
    "## clean duplicates\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "\n",
    "duplicates = 0\n",
    "to_be_removed = []\n",
    "\n",
    "for i, problem_i in enumerate(problems):\n",
    "    print(\"Cleaning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "    for problem_j in problems[i + 1:]:\n",
    "        if filecmp.cmp('./dataset/problems/' + problem_i, \n",
    "                       './dataset/problems/' + problem_j, \n",
    "                       shallow=False):\n",
    "            duplicates += 1\n",
    "            to_be_removed.append(problem_j)\n",
    "            \n",
    "for problem in to_be_removed:\n",
    "    try:\n",
    "        os.remove('./dataset/problems/' + problem)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "            \n",
    "print(str(duplicates) + \" duplicates deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plans with Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning --- 3.1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::bad_alloc'\n",
      "  what():  std::bad_alloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    \n",
    "    result = subprocess.run(['./generator/probe', \n",
    "                             '-d', './dataset/domains/domain_m_25_B.pddl', \n",
    "                             '-i', './dataset/problems/' + problem,\n",
    "                             '-o', './dataset/plans/m_25_B-' + str(problem.split('_',2)[2:][0])], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    print(\"Planning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "    \n",
    "    # temporarly limit to 400 samples\n",
    "    if i > 500: break\n",
    "\n",
    "print(\"Planning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Probe plans to IPC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plans converted to IPC format\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "plans = sorted(os.listdir('./dataset/plans/'))\n",
    "\n",
    "for plan in plans:\n",
    "    probe = open('./dataset/plans/' + plan, \"r\")\n",
    "    val = open('./dataset/plans_ipc/' + plan.split(\".\")[0] + \".plan\", 'w+')\n",
    "    replacement = \"\"\n",
    "    # using the for loop\n",
    "    action_timing = 1\n",
    "    for line in probe:\n",
    "        line = line.lower()\n",
    "        line = '%.3f' % (action_timing / 1000) + \"00: \" + line\n",
    "        val.write(line)\n",
    "        action_timing += 2\n",
    "\n",
    "    probe.close()\n",
    "    val.close()\n",
    "    \n",
    "print(\"Plans converted to IPC format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and remove invalid plans using VAL Validate\n",
    "This step takes a lot of time and usually all plans are validated. **It can be skipped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 plans could not be validated and have been removed from the dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "plans = sorted(os.listdir('./dataset/plans_ipc/'))\n",
    "invalid_plans = 0\n",
    "\n",
    "for i, plan in enumerate(plans):\n",
    "    result = subprocess.run(['./generator/Validate', \n",
    "                             './dataset/domains/domain_m_25_B.pddl', \n",
    "                             './dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\",\n",
    "                             './dataset/plans_ipc/' + plan], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    if \"Plan failed to execute\" in str(result):\n",
    "        invalid_plans += 1\n",
    "#         os.remove('./dataset/plans_ipc/m_25_B-' + problem.split('_',2)[2:][0].split(\".\")[0] + \".plan\")\n",
    "#         os.remove('./dataset/problems/' + problem)\n",
    "\n",
    "    print(\"Validating --- \" + str(round((i / 8000 * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print('%i plans could not be validated and have been removed from the dataset' % invalid_plans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile GPT3 training dataset\n",
    "TODO: `joint3)\\n)\\n)\\n` is seen as part of the prompt\n",
    "\n",
    "From the documentation regarding conditional generation:\n",
    "* Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.\n",
    "* Use an ending token at the end of the completion, e.g. END\n",
    "* Remember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]\n",
    "\n",
    "Example:\n",
    "```{\"prompt\":\"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\", \"completion\":\" <engaging ad> END\"}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready!\n",
      "Before training, run in a terminal\n",
      "\t>openai tools fine_tunes.prepare_data -f './dataset/formatted/teriyaki.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "dataset = open('./dataset/formatted/teriyaki.jsonl', 'w+')\n",
    "plans = sorted(os.listdir('./dataset/plans_ipc/'))\n",
    "\n",
    "# extracting prompts from problems and completions from IPC plans \n",
    "for i, plan in enumerate(plans):\n",
    "    plan_f = open('./dataset/plans_ipc/' + plan, \"r\")\n",
    "    problem_f = open('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \"r\")\n",
    "    lines = problem_f.readlines()\n",
    "    \n",
    "    # lines[49:62] is the range of meaningful predicates at initial state\n",
    "    dataset.write(repr('{\"prompt\":\"\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + \n",
    "          '\\n\\n###\\n\\n\", \"completion\":\" ' + plan_f.read().replace(\"    \", \"\") + ' END\"}')[1:-1] + \"\\n\")\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "print(\"Dataset ready!\")\n",
    "print(\"Before training, run in a terminal\\n\\t>openai tools fine_tunes.prepare_data -f './dataset/formatted/teriyaki.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute training cost\n",
    "`1 token ~= 4 chars\n",
    "0.0300$/1K Tokens`\n",
    "\n",
    "In practice it is ~15$ per 1000 samples for this specific planning domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training this dataset will consume about 164945.75 tokens for an estimated cost of 4.95$\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset = open('./dataset/formatted/teriyaki.jsonl', 'r')\n",
    "chars = len(dataset.read())\n",
    "dataset.close()\n",
    "tokens = chars / 4\n",
    "cost = tokens * 0.00003\n",
    "\n",
    "print(\"Training this dataset will consume about {0} tokens for an estimated cost of {1}$\".format(tokens, round(cost,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a customized model and training!\n",
    "Model: **ft-dVpnKEC46BV9odOlRsGLO7nd** - **davinci:ft-personal-2022-08-18-17-42-22**\n",
    "\n",
    "From the documentation regarding conditional generation:\n",
    "* Aim for at least ~500 examples\n",
    "* Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n",
    "* Ensure the examples are of high quality and follow the same desired format\n",
    "* Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n",
    "* Using Lower learning rate and only 1-2 epochs tends to work better for these use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████████████████| 660k/660k [00:00<00:00, 1.02Git/s]\n",
      "Uploaded file from ./dataset/formatted/teriyaki.jsonl: file-IvVN6w5cZHMNmx6i5o5FHBs3\n",
      "Created fine-tune: ft-dVpnKEC46BV9odOlRsGLO7nd\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2022-08-18 19:33:20] Created fine-tune: ft-dVpnKEC46BV9odOlRsGLO7nd\n",
      "[2022-08-18 19:33:30] Fine-tune costs $7.00\n",
      "[2022-08-18 19:33:31] Fine-tune enqueued. Queue number: 0\n",
      "[2022-08-18 19:33:33] Fine-tune started\n",
      "[2022-08-18 19:41:43] Completed epoch 1/1\n",
      "[2022-08-18 19:42:22] Uploaded model: davinci:ft-personal-2022-08-18-17-42-22\n",
      "[2022-08-18 19:42:24] Uploaded result file: file-mfdWBbs4HOcYtTf3WJoLCtI5\n",
      "[2022-08-18 19:42:24] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m davinci:ft-personal-2022-08-18-17-42-22 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.create -t './dataset/formatted/teriyaki.jsonl' -m 'davinci' --n_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.00100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.00200: (link-to-central-grasp link2 link1 joint3 joint1 gleft gright)\n",
      "0.00300: (decrease_angle_first_child link2 link1 joint1 angle0 angle345 gleft gright)\n",
      "0.00400: (decrease_angle_first_child link2 link1 joint1 angle345 angle330 gleft gright)\n",
      "0.00500: (decrease_angle_first_child link2 link1 joint1 angle330 angle315 gleft gright)\n",
      "0.00600: (release-links link2 link1 joint1 gleft gright)\n",
      "0.00700: (link-to-central-grasp link3 link2 joint1 joint2 gleft gright)\n",
      "0.00800: (increase_angle_first_child_45 link3 link2 joint2 angle270 angle285 angle300 angle315 gleft gright)\n",
      "0.00900: (release-links link3 link2 joint2 gleft gright)\n",
      "0.01000: (link-to-central-grasp link4 link3 joint2 joint3 gleft gright)\n",
      "0.01100: (decrease_angle_first_child link4 link3 joint3 angle345 angle330 gleft gright)\n",
      "0.01200: (decrease_angle_first_child link4 link3 joint3 angle330 angle315 gleft gright)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# !openai -k ***REMOVED*** \\\n",
    "# api completions.create -m 'davinci:ft-personal-2022-08-18-17-42-22' \\\n",
    "# -p '(:init\\n(angle_joint angle0 joint1)\\n(angle_joint angle345 joint2)\\n(angle_joint angle0 joint3)\\n(in-centre joint3)\\n(in-hand link1)\\n(in-hand link2)\\n(grasp gleft link1)\\n(grasp gright link2))\\n(:goal(and\\n(angle_joint angle285 joint1)\\n(angle_joint angle285 joint2)\\n(angle_joint angle315 joint3)\\n)\\n)\\n)\\n\\n\\n###\\n\\n'\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-personal-2022-08-18-17-42-22\",\n",
    "                                        prompt =  texts,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 400,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "problem = '(:init\\n(angle_joint angle0 joint1)\\n(angle_joint angle345 joint2)\\n(angle_joint angle0 joint3)\\n(in-centre joint3)\\n(in-hand link1)\\n(in-hand link2)\\n(grasp gleft link1)\\n(grasp gright link2))\\n(:goal(and\\n(angle_joint angle285 joint1)\\n(angle_joint angle285 joint2)\\n(angle_joint angle315 joint3)\\n)\\n)\\n)\\n\\n\\n###\\n\\n'\n",
    "\n",
    "GPT_Completion(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run extended tests on the dataset\n",
    "Plan with Teriyaki for all available problems and run VAL Validate\n",
    "\n",
    "It reports:\n",
    "* success rate\n",
    "* min, max, avg and standard deviation of planning times\n",
    "* average number of actions before Teryiaki makes a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative planning trick\n",
    "* Try to plan only N actions \n",
    "* Use VAL **ValStep** to simulate the outcomes of said actions and save it to a problem file\n",
    "* Replan using the new problem file\n",
    "* Iterate until goal state is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
