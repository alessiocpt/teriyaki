{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Apple Chicken:\n",
      "\n",
      "1. Preheat oven to 375 degrees.\n",
      "2. Coat chicken in flour.\n",
      "3. Place chicken in a baking dish.\n",
      "4. Cut apples into thin slices and place on top of chicken.\n",
      "5. Sprinkle with salt.\n",
      "6. Bake for 30 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"text-davinci-002\",\n",
    "                                        prompt =  texts,\n",
    "                                        temperature = 0.6,\n",
    "                                        top_p = 1,\n",
    "                                        max_tokens = 64,\n",
    "                                        frequency_penalty = 0,\n",
    "                                        presence_penalty = 0)\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "recipe = 'Provide a cooking recipe based on the following ingredients: \\\\n \\nApple \\\n",
    "          \\n \\nFlour \\\n",
    "          \\n \\nChicken \\\n",
    "          \\n \\nSalt'\n",
    "\n",
    "GPT_Completion(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conditional problem files to /home/alessio/planning/teriyaki/generator/problems/conditional...\n",
      "Done.\n",
      "All operations completed. Happy planning!\n",
      "40 duplicates deleted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import filecmp\n",
    "\n",
    "## cleaning work directories \n",
    "!rm dataset/problems/\\* &> /dev/null\n",
    "!rm dataset/plans/\\* &> /dev/null\n",
    "!rm dataset/plans_ipc/\\* &> /dev/null\n",
    "!rm dataset/formatted/\\* &> /dev/null\n",
    "\n",
    "## 8000 data points should improve performance between 2x and 4x\n",
    "%run generator/pddl_confgen.py 4 9000 -r 24 -co\n",
    "!mv -v generator/problems/conditional/* dataset/problems > /dev/null\n",
    "\n",
    "## clean duplicates\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "\n",
    "duplicates = 0\n",
    "to_be_removed = []\n",
    "\n",
    "for i, problem_i in enumerate(problems):\n",
    "    print(\"Cleaning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "    for problem_j in problems[i + 1:]:\n",
    "        if filecmp.cmp('./dataset/problems/' + problem_i, \n",
    "                       './dataset/problems/' + problem_j, \n",
    "                       shallow=False):\n",
    "            duplicates += 1\n",
    "            to_be_removed.append(problem_j)\n",
    "            \n",
    "for problem in to_be_removed:\n",
    "    try:\n",
    "        os.remove('./dataset/problems/' + problem)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "            \n",
    "print(str(duplicates) + \" duplicates deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plans with Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning --- 34.4%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::bad_alloc'\n",
      "  what():  std::bad_alloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning --- 95.1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::bad_alloc'\n",
      "  what():  std::bad_alloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning complete0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    \n",
    "    result = subprocess.run(['./generator/probe', \n",
    "                             '-d', './dataset/domains/domain_m_25_B.pddl', \n",
    "                             '-i', './dataset/problems/' + problem,\n",
    "                             '-o', './dataset/plans/m_25_B-' + str(problem.split('_',2)[2:][0])], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    print(\"Planning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "    \n",
    "#     # temporarly limit to 400 samples\n",
    "#     if i > 500: break\n",
    "\n",
    "print(\"Planning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Probe plans to IPC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plans converted to IPC format\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "plans = sorted(os.listdir('./dataset/plans/'))\n",
    "\n",
    "for plan in plans:\n",
    "    probe = open('./dataset/plans/' + plan, \"r\")\n",
    "    val = open('./dataset/plans_ipc/' + plan.split(\".\")[0] + \".plan\", 'w+')\n",
    "    replacement = \"\"\n",
    "    # using the for loop\n",
    "    action_timing = 1\n",
    "    for line in probe:\n",
    "        line = line.lower()\n",
    "        line = '%.3f' % (action_timing / 1000) + \"00: \" + line\n",
    "        val.write(line)\n",
    "        action_timing += 2\n",
    "\n",
    "    probe.close()\n",
    "    val.close()\n",
    "    \n",
    "print(\"Plans converted to IPC format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and remove invalid plans using VAL Validate\n",
    "This step takes a lot of time and usually all plans are validated. **It can be skipped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 plans could not be validated and have been removed from the dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "plans = sorted(os.listdir('./dataset/plans_ipc/'))\n",
    "invalid_plans = 0\n",
    "\n",
    "for i, plan in enumerate(plans):\n",
    "    result = subprocess.run(['./generator/Validate', \n",
    "                             './dataset/domains/domain_m_25_B.pddl', \n",
    "                             './dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\",\n",
    "                             './dataset/plans_ipc/' + plan], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    if \"Plan failed to execute\" in str(result):\n",
    "        invalid_plans += 1\n",
    "#         os.remove('./dataset/plans_ipc/m_25_B-' + problem.split('_',2)[2:][0].split(\".\")[0] + \".plan\")\n",
    "#         os.remove('./dataset/problems/' + problem)\n",
    "\n",
    "    print(\"Validating --- \" + str(round((i / len(plans) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print('%i plans could not be validated and have been removed from the dataset' % invalid_plans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort problem & plans in batch folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All problems and plans have been sorted into batches.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "## cleaning work directories \n",
    "!rm dataset/batches/500/problems/* &> /dev/null\n",
    "!rm dataset/batches/500/plans/* &> /dev/null\n",
    "!rm dataset/batches/1000/problems/* &> /dev/null\n",
    "!rm dataset/batches/1000/plans/* &> /dev/null\n",
    "!rm dataset/batches/2000/problems/* &> /dev/null\n",
    "!rm dataset/batches/2000/plans/* &> /dev/null\n",
    "!rm dataset/batches/4000/problems/* &> /dev/null\n",
    "!rm dataset/batches/4000/plans/* &> /dev/null\n",
    "!rm dataset/batches/8000/problems/* &> /dev/null\n",
    "!rm dataset/batches/8000/plans/* &> /dev/null\n",
    "!rm dataset/batches/validation/problems/* &> /dev/null\n",
    "!rm dataset/batches/validation/plans/* &> /dev/null\n",
    "\n",
    "## fetching and sorting problems and plans\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "plans = sorted(os.listdir('./dataset/plans_ipc/'))\n",
    "\n",
    "for i, plan in enumerate(plans):\n",
    "    if i < 500:\n",
    "        shutil.copy2('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/500/problems/')\n",
    "        shutil.copy2('./dataset/plans_ipc/' + plans[i], 'dataset/batches/500/plans/')\n",
    "    elif i < 1000:\n",
    "        shutil.copy2('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/1000/problems/')\n",
    "        shutil.copy2('./dataset/plans_ipc/' + plans[i], 'dataset/batches/1000/plans/')\n",
    "    elif i < 2000:\n",
    "        shutil.copy2('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/2000/problems/')\n",
    "        shutil.copy2('./dataset/plans_ipc/' + plans[i], 'dataset/batches/2000/plans/')\n",
    "    elif i < 4000:\n",
    "        shutil.copy2('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/4000/problems/')\n",
    "        shutil.copy2('./dataset/plans_ipc/' + plans[i], 'dataset/batches/4000/plans/')\n",
    "    elif i < 8000:\n",
    "        shutil.copy2('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/8000/problems/')\n",
    "        shutil.copy2('./dataset/plans_ipc/' + plans[i], 'dataset/batches/8000/plans/')\n",
    "    else:\n",
    "        shutil.copy2('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/validation/problems/')\n",
    "        shutil.copy2('./dataset/plans_ipc/' + plans[i], 'dataset/batches/validation/plans/')\n",
    "\n",
    "print('All problems and plans have been sorted into batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile GPT3 training datasets\n",
    "\n",
    "This step will compile `jsonl` for all training batches. Later you can use each of them to increment the fine tuning dataset size progressively.\n",
    "\n",
    "Unfortunately, before use you need to manually run in a terminal the following line for each of them:\n",
    "\n",
    "```>openai tools fine_tunes.prepare_data -f './dataset/batches/[YOUR_BATCH]/teriyaki.jsonl```\n",
    "\n",
    "=====================================================\n",
    "\n",
    "TODO: `joint3)\\n)\\n)\\n` is seen as part of the prompt\n",
    "\n",
    "From the documentation regarding conditional generation:\n",
    "* Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.\n",
    "* Use an ending token at the end of the completion, e.g. END\n",
    "* Remember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]\n",
    "\n",
    "Example:\n",
    "```{\"prompt\":\"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\", \"completion\":\" <engaging ad> END\"}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready!\n",
      "Before training, run in a terminal\n",
      "\t>openai tools fine_tunes.prepare_data -f './dataset/batch/[YOUR_BATCH]/teriyaki.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "batches = [\"500\", \"1000\", \"2000\", \"4000\", \"8000\", \"validation\"]\n",
    "\n",
    "for batch in batches:\n",
    "    dataset = open('./dataset/batches/' + batch + '/teriyaki.jsonl', 'w+')\n",
    "    plans = sorted(os.listdir('./dataset/batches/' + batch + '/plans/'))\n",
    "\n",
    "    # extracting prompts from problems and completions from IPC plans \n",
    "    for i, plan in enumerate(plans):\n",
    "        plan_f = open('./dataset/batches/' + batch + '/plans/' + plan, \"r\")\n",
    "        problem_f = open('./dataset/batches/' + batch + '/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \"r\")\n",
    "        lines = problem_f.readlines()\n",
    "\n",
    "        # lines[49:62] is the range of meaningful predicates at initial state\n",
    "        dataset.write(repr('{\"prompt\":\"\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + \n",
    "              '\\n\\n###\\n\\n\", \"completion\":\" ' + plan_f.read().replace(\"    \", \"\") + ' END\"}')[1:-1] + \"\\n\")\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "print(\"Datasets ready!\")\n",
    "print(\"Before training, run in a terminal\\n\\t>openai tools fine_tunes.prepare_data -f './dataset/batches/[YOUR_BATCH]/teriyaki.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute training cost\n",
    "`1 token ~= 4 chars\n",
    "0.0300$/1K Tokens`\n",
    "\n",
    "In practice it is ~15$ per 1000 samples for this specific planning domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training this dataset will consume about 157412.5 tokens for an estimated cost of 4.72$\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset = open('./dataset/batches/500/teriyaki.jsonl', 'r')\n",
    "chars = len(dataset.read())\n",
    "dataset.close()\n",
    "tokens = chars / 4\n",
    "cost = tokens * 0.00003\n",
    "\n",
    "print(\"Training this dataset will consume about {0} tokens for an estimated cost of {1}$\".format(tokens, round(cost,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a fine-tuned model!\n",
    "\n",
    "### <ins>RUN ONCE</ins>\n",
    "\n",
    "### Last model\n",
    "Job **ft-PXtNnUR8jjnAzIgW1YhRpGrt**\n",
    "\n",
    "Model **davinci:ft-teriyaki:teriyaki-001-2022-11-30-18-48-47**\n",
    "\n",
    "### Dataset \n",
    "From the documentation regarding conditional generation:\n",
    "* Aim for at least ~500 examples\n",
    "* Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n",
    "* Ensure the examples are of high quality and follow the same desired format\n",
    "* Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n",
    "* Using Lower learning rate and only 1-2 epochs tends to work better for these use cases\n",
    "\n",
    "LEARN MORE ABOUT VALIDATION, DELETING, AND SUFFIX @ \n",
    "\n",
    "https://beta.openai.com/docs/guides/fine-tuning/advanced-usage\n",
    "\n",
    "https://beta.openai.com/docs/api-reference/fine-tunes\n",
    "\n",
    "### Training hyperparameters\n",
    "* `model` use `'davinci'` or `'code-davinci-002'` (CODEX, not yet supported)\n",
    "* `n_epochs` defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. For generative use cases 2 epochs is generally better.\n",
    "* `batch_size` defaults to ~0.2% of the number of examples in the training set, capped at 256. Larger batch sizes tend to work better for larger datasets.\n",
    "* `learning_rate_multiplier`defaults to 0.05, 0.1, or 0.2 depending on final `batch_size`. We recommend experimenting with values in the range 0.02 to 0.2. Larger learning rates often perform better with larger batch sizes.\n",
    "* `compute_classification_metrics`defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.\n",
    "\n",
    "### Results\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:\n",
    "\n",
    "* `elapsed_tokens`: the number of tokens the model has seen so far (including repeats)\n",
    "* `elapsed_examples`: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.\n",
    "* `training_loss`: loss on the training batch\n",
    "* `training_sequence_accuracy`: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n",
    "* `training_token_accuracy`: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-30 23:20:17] Created fine-tune: ft-JNw1L5bGAnUwJdxSXQAQrdbj\n",
      "[2022-11-30 23:20:25] Fine-tune costs $109.78\n",
      "[2022-11-30 23:20:25] Fine-tune enqueued. Queue number: 5\n",
      "[2022-11-30 23:25:17] Fine-tune is in the queue. Queue number: 4\n",
      "[2022-12-01 00:08:39] Fine-tune is in the queue. Queue number: 3\n",
      "[2022-12-01 00:12:43] Fine-tune is in the queue. Queue number: 2\n",
      "[2022-12-01 00:13:08] Fine-tune is in the queue. Queue number: 1\n",
      "[2022-12-01 00:22:56] Fine-tune is in the queue. Queue number: 0\n",
      "[2022-12-01 00:24:34] Fine-tune started\n",
      "[2022-12-01 00:56:47] Completed epoch 1/2\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-JNw1L5bGAnUwJdxSXQAQrdbj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# !openai -k ***REMOVED*** \\\n",
    "# api fine_tunes.create -t './dataset/batches/500/teriyaki.jsonl' \\\n",
    "# -v './dataset/batches/validation/teriyaki.jsonl' \\\n",
    "# -m 'davinci' --suffix \"teriyaki-001\" --n_epochs 2\n",
    "\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.follow -i ft-JNw1L5bGAnUwJdxSXQAQrdbj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-PXtNnUR8jjnAzIgW1YhRpGrt > dataset/batches/500/results.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-EFuAgXbaOpytBR8VGcIq4Qfe > dataset/batches/1000/results.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-OkqNqBWMm0ATjHB7r2Prsgfj > dataset/batches/2000/results.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-cldwwOoDt4xGY2WGtTxJo5yh > dataset/batches/4000/results.csv\n",
    "\n",
    "# response = !openai -k ***REMOVED*** \\\n",
    "# api fine_tunes.results -i ft-PXtNnUR8jjnAzIgW1YhRpGrt > dataset/batches/8000/results.csv\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further tuning\n",
    "\n",
    "### <ins>Insert previous model and run in order for each batch at most once!</ins>\n",
    "\n",
    "* **1000** ft-EFuAgXbaOpytBR8VGcIq4Qfe - ft-teriyaki:teriyaki-001-2022-11-30-19-48-12\n",
    "* **2000** ft-OkqNqBWMm0ATjHB7r2Prsgfj - ft-teriyaki:teriyaki-001-2022-11-30-21-21-23\n",
    "* **4000** ft-cldwwOoDt4xGY2WGtTxJo5yh - ft-teriyaki:teriyaki-001-2022-11-30-22-07-22\n",
    "* **8000** ft-JNw1L5bGAnUwJdxSXQAQrdbj - \n",
    "\n",
    "Not our case, but...\n",
    "\n",
    "```if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|███████████████████████| 646k/646k [00:00<00:00, 307Mit/s]\n",
      "Uploaded file from ./dataset/batches/1000/teriyaki.jsonl: file-WkGWR3hD1sofGiz4lDaFCOJ9\n",
      "Found potentially duplicated files with name 'teriyaki.jsonl', purpose 'fine-tune' and size 1101044 bytes\n",
      "file-xQOM754uLyXXJezwzs1BLlSZ\n",
      "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "batches = [\"1000\", \"2000\", \"4000\", \"8000\"]\n",
    "\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.create -t './dataset/batches/1000/teriyaki.jsonl' \\\n",
    "-v './dataset/batches/validation/teriyaki.jsonl' \\\n",
    "-m 'davinci:ft-teriyaki:teriyaki-001-2022-11-30-18-48-47' --suffix \"teriyaki-001\" --n_epochs 2\n",
    "\n",
    "# api fine_tunes.follow -i ft-PXtNnUR8jjnAzIgW1YhRpGrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.00100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.00300: (link-to-central-grasp link2 link1 joint3 joint1 gleft gright)\n",
      "0.00500: (decrease_angle_first_child_45 link2 link1 joint1 angle0 angle345 angle330 angle315 gleft gright)\n",
      "0.00700: (decrease_angle_first_child link2 link1 joint1 angle315 angle300 gleft gright)\n",
      "0.00900: (decrease_angle_first_child link2 link1 joint1 angle300 angle285 gleft gright)\n",
      "0.01100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.01300: (link-to-central-grasp link3 link2 joint1 joint2 gleft gright)\n",
      "0.01500: (increase_angle_first_child link3 link2 joint2 angle270 angle285 gleft gright)\n",
      "0.01700: (release-links link3 link2 joint2 gleft gright)\n",
      "0.01900: (link-to-central-grasp link4 link3 joint2 joint3 gleft gright)\n",
      "0.02100: (increase_angle_first_child link4 link3 joint3 angle300 angle315 gleft gright)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-11-30-22-07-22\",\n",
    "                                        prompt =  texts,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 600,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "problem = '(:init\\n(angle_joint angle0 joint1)\\n(angle_joint angle345 joint2)\\n(angle_joint angle0 joint3)\\n(in-centre joint3)\\n(in-hand link1)\\n(in-hand link2)\\n(grasp gleft link1)\\n(grasp gright link2))\\n(:goal(and\\n(angle_joint angle285 joint1)\\n(angle_joint angle285 joint2)\\n(angle_joint angle315 joint3)\\n)\\n)\\n)\\n\\n\\n###\\n\\n'\n",
    "\n",
    "GPT_Completion(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run extended tests on the dataset\n",
    "Plan with Teriyaki for all available problems and run VAL Validate\n",
    "\n",
    "It reports:\n",
    "* success rate\n",
    "* min, max, avg and standard deviation of planning times\n",
    "* average number of actions before Teryiaki makes a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-personal-2022-08-18-17-42-22\",\n",
    "                                        prompt =  texts,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 400,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    return response.choices[0].text\n",
    "\n",
    "batches = [\"500\", \"1000\", \"2000\", \"4000\", \"8000\"]\n",
    "\n",
    "## is planning needed or already in GPT-3 validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative planning trick\n",
    "* Try to plan only N actions \n",
    "* Use VAL **ValStep** to simulate the outcomes of said actions and save it to a problem file\n",
    "* Replan using the new problem file\n",
    "* Iterate until goal state is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
