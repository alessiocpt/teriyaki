{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Apple Chicken:\n",
      "\n",
      "1. Preheat oven to 375 degrees.\n",
      "2. Coat chicken in flour.\n",
      "3. Place chicken in a baking dish.\n",
      "4. Cut apples into thin slices and place on top of chicken.\n",
      "5. Sprinkle with salt.\n",
      "6. Bake for 30 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"text-davinci-002\",\n",
    "                                        prompt =  texts,\n",
    "                                        temperature = 0.6,\n",
    "                                        top_p = 1,\n",
    "                                        max_tokens = 64,\n",
    "                                        frequency_penalty = 0,\n",
    "                                        presence_penalty = 0)\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "recipe = 'Provide a cooking recipe based on the following ingredients: \\\\n \\nApple \\\n",
    "          \\n \\nFlour \\\n",
    "          \\n \\nChicken \\\n",
    "          \\n \\nSalt'\n",
    "\n",
    "GPT_Completion(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!mkdir dataset/problems/\n",
    "!mkdir dataset/plans/\n",
    "!mkdir dataset/plans_ipc/\n",
    "!mkdir dataset/formatted/\n",
    "!mkdir dataset/plans_ipc/\n",
    "!mkdir dataset/batches\n",
    "!mkdir dataset/batches/500/problems\n",
    "!mkdir dataset/batches/500/plans\n",
    "!mkdir dataset/batches/1000/problems\n",
    "!mkdir dataset/batches/1000/plans\n",
    "!mkdir dataset/batches/2000/problems\n",
    "!mkdir dataset/batches/2000/plans\n",
    "!mkdir dataset/batches/4000/problems\n",
    "!mkdir dataset/batches/4000/plans\n",
    "!mkdir dataset/batches/8000/problems\n",
    "!mkdir dataset/batches/8000/plans\n",
    "!mkdir dataset/batches/validation/problems\n",
    "!mkdir dataset/batches/validation/plans\n",
    "!mkdir dataset/batches/validation/results/500\n",
    "!mkdir dataset/batches/validation/results/1000\n",
    "!mkdir dataset/batches/validation/results/2000\n",
    "!mkdir dataset/batches/validation/results/4000\n",
    "!mkdir dataset/batches/validation/results/8000\n",
    "!mkdir dataset/batches/validation/results/500\n",
    "!mkdir dataset/batches/validation/results/500\n",
    "!mkdir dataset/batches/validation/results/1000/plans\n",
    "!mkdir dataset/batches/validation/results/1000/val\n",
    "!mkdir dataset/batches/validation/results/2000/plans\n",
    "!mkdir dataset/batches/validation/results/2000/val\n",
    "!mkdir dataset/batches/validation/results/4000/plans\n",
    "!mkdir dataset/batches/validation/results/4000/val\n",
    "!mkdir dataset/batches/validation/results/8000/plans\n",
    "!mkdir dataset/batches/validation/results/8000/val\n",
    "!mkdir dataset/batches/validation/unsolved\n",
    "!mkdir dataset/batches/validation/unsolved/problems\n",
    "!mkdir dataset/batches/validation/unsolved/plans\n",
    "!mkdir dataset/batches/validation/unsolved/val_plans\n",
    "!mkdir dataset/batches/validation/unsolved/val\n",
    "!mkdir dataset/test/\n",
    "!mkdir dataset/test/problems\n",
    "!mkdir dataset/test/plans\n",
    "!mkdir dataset/test/val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conditional problem files to /home/alessio/planning/teriyaki/generator/problems/conditional...\n",
      "Done.\n",
      "All operations completed. Happy planning!\n",
      "40 duplicates deleted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import filecmp\n",
    "\n",
    "## cleaning work directories \n",
    "!rm dataset/problems/* &> /dev/null\n",
    "!rm dataset/plans/* &> /dev/null\n",
    "!rm dataset/plans_ipc/* &> /dev/null\n",
    "!rm dataset/formatted/* &> /dev/null\n",
    "\n",
    "## 8000 data points should improve performance between 2x and 4x\n",
    "%run generator/pddl_confgen.py 4 9000 -r 24 -co\n",
    "!mv -v generator/problems/conditional/* dataset/problems > /dev/null\n",
    "\n",
    "## clean duplicates\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "\n",
    "duplicates = 0\n",
    "to_be_removed = []\n",
    "\n",
    "for i, problem_i in enumerate(problems):\n",
    "    print(\"Cleaning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "    for problem_j in problems[i + 1:]:\n",
    "        if filecmp.cmp('./dataset/problems/' + problem_i, \n",
    "                       './dataset/problems/' + problem_j, \n",
    "                       shallow=False):\n",
    "            duplicates += 1\n",
    "            to_be_removed.append(problem_j)\n",
    "            \n",
    "for problem in to_be_removed:\n",
    "    try:\n",
    "        os.remove('./dataset/problems/' + problem)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "            \n",
    "print(str(duplicates) + \" duplicates deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plans with Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning --- 34.4%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::bad_alloc'\n",
      "  what():  std::bad_alloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning --- 95.1%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::bad_alloc'\n",
      "  what():  std::bad_alloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning complete0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    \n",
    "    result = subprocess.run(['./generator/probe', \n",
    "                             '-d', './dataset/domains/domain_m_25_B.pddl', \n",
    "                             '-i', './dataset/problems/' + problem,\n",
    "                             '-o', './dataset/plans/m_25_B-' + str(problem.split('_',2)[2:][0])], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    print(\"Planning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "    \n",
    "#     # temporarly limit to 400 samples\n",
    "#     if i > 500: break\n",
    "\n",
    "print(\"Planning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Probe plans to IPC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plans converted to IPC format\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# clean working directories\n",
    "!rm dataset/plans_ipc/* &> /dev/null\n",
    "\n",
    "plans = sorted(os.listdir('./dataset/plans/'))\n",
    "\n",
    "for plan in plans:\n",
    "    probe = open('./dataset/plans/' + plan, \"r\")\n",
    "    val = open('./dataset/plans_ipc/' + plan.split(\".\")[0] + \".plan\", 'w+')\n",
    "    replacement = \"\"\n",
    "    # using the for loop\n",
    "    action_timing = 1\n",
    "    for line in probe:\n",
    "        line = line.lower()\n",
    "        line = '%.3f' % (action_timing / 1000) + \"00: \" + line\n",
    "        val.write(line)\n",
    "        action_timing += 2\n",
    "\n",
    "    probe.close()\n",
    "    val.close()\n",
    "    \n",
    "print(\"Plans converted to IPC format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and remove invalid plans using VAL Validate\n",
    "This step takes a lot of time and usually all plans are validated. **It can be skipped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965 plans could not be validated and have been removed from the dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "plans = sorted(os.listdir('./dataset/plans_ipc/'))\n",
    "invalid_plans = 0\n",
    "\n",
    "for i, plan in enumerate(plans):\n",
    "    result = subprocess.run(['./generator/Validate', \n",
    "                             './dataset/domains/domain_m_25_B.pddl', \n",
    "                             './dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\",\n",
    "                             './dataset/plans_ipc/' + plan], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    if \"Plan failed to execute\" in str(result):\n",
    "        invalid_plans += 1\n",
    "#         os.remove('./dataset/plans_ipc/m_25_B-' + problem.split('_',2)[2:][0].split(\".\")[0] + \".plan\")\n",
    "#         os.remove('./dataset/problems/' + problem)\n",
    "\n",
    "    print(\"Validating --- \" + str(round((i / len(plans) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print('%i plans could not be validated and have been removed from the dataset' % invalid_plans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort problem & plans in batch folders\n",
    "By deafult, we created several datasets so that the total number of samples used for fine tuning doubles aeach time, starting from the minimum 500 samples up to the recommendend 8000 maximum.\n",
    "\n",
    "We also create an additional validation batch of ~1000 sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘dataset/batches/validation/unsolved’: File exists\n",
      "mkdir: cannot create directory ‘dataset/batches/validation/unsolved/problems’: File exists\n",
      "mkdir: cannot create directory ‘dataset/batches/validation/unsolved/plans’: File exists\n",
      "All problems and plans have been sorted into batches.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "## cleaning work directories \n",
    "!rm dataset/batches/500/problems/* &> /dev/null\n",
    "!rm dataset/batches/500/plans/* &> /dev/null\n",
    "!rm dataset/batches/1000/problems/* &> /dev/null\n",
    "!rm dataset/batches/1000/plans/* &> /dev/null\n",
    "!rm dataset/batches/2000/problems/* &> /dev/null\n",
    "!rm dataset/batches/2000/plans/* &> /dev/null\n",
    "!rm dataset/batches/4000/problems/* &> /dev/null\n",
    "!rm dataset/batches/4000/plans/* &> /dev/null\n",
    "!rm dataset/batches/8000/problems/* &> /dev/null\n",
    "!rm dataset/batches/8000/plans/* &> /dev/null\n",
    "!rm dataset/batches/validation/problems/* &> /dev/null\n",
    "!rm dataset/batches/validation/plans/* &> /dev/null\n",
    "!rm dataset/batches/validation/unsolved/problems/* &> /dev/null\n",
    "!rm dataset/batches/validation/unsolved/plans/* &> /dev/null\n",
    "!rm dataset/batches/validation/unsolved/val/* &> /dev/null\n",
    "!rm dataset/batches/validation/unsolved/plans_val/* &> /dev/null\n",
    "\n",
    "## fetching and sorting problems and plans\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "plans = sorted(os.listdir('./dataset/plans_ipc/'))\n",
    "\n",
    "for i, plan in enumerate(plans):\n",
    "    if i < 500:\n",
    "        shutil.move('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     './dataset/batches/500/problems/')\n",
    "        shutil.move('./dataset/plans_ipc/' + plans[i], './dataset/batches/500/plans/')\n",
    "    elif i < 1000:\n",
    "        shutil.move('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     './dataset/batches/1000/problems/')\n",
    "        shutil.move('./dataset/plans_ipc/' + plans[i], './dataset/batches/1000/plans/')\n",
    "    elif i < 2000:\n",
    "        shutil.move('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     './dataset/batches/2000/problems/')\n",
    "        shutil.move('./dataset/plans_ipc/' + plans[i], './dataset/batches/2000/plans/')\n",
    "    elif i < 4000:\n",
    "        shutil.move('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     './dataset/batches/4000/problems/')\n",
    "        shutil.move('./dataset/plans_ipc/' + plans[i], './dataset/batches/4000/plans/')\n",
    "    elif i < 8000:\n",
    "        shutil.move('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     './dataset/batches/8000/problems/')\n",
    "        shutil.move('./dataset/plans_ipc/' + plans[i], './dataset/batches/8000/plans/')\n",
    "    else:\n",
    "        shutil.move('./dataset/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     './dataset/batches/validation/problems/')\n",
    "        shutil.move('./dataset/plans_ipc/' + plans[i], './dataset/batches/validation/plans/')\n",
    "\n",
    "## move unsolved problems to the validation folder to be attempted later by Teriyaki\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/problems/'))\n",
    "for problem in problems:\n",
    "    shutil.move('./dataset/problems/' + problem, './dataset/batches/validation/unsolved/problems/' + problem)\n",
    "    \n",
    "print('All problems and plans have been sorted into batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile GPT3 training datasets\n",
    "\n",
    "This step will compile `jsonl` for all training batches. Later you can use each of them to increment the fine tuning dataset size progressively.\n",
    "\n",
    "Unfortunately, before use you need to manually run in a terminal the following line for each of them:\n",
    "\n",
    "```>openai tools fine_tunes.prepare_data -f './dataset/batches/[YOUR_BATCH]/teriyaki.jsonl```\n",
    "\n",
    "=====================================================\n",
    "\n",
    "TODO: `joint3)\\n)\\n)\\n` is seen as part of the prompt\n",
    "\n",
    "From the documentation regarding conditional generation:\n",
    "* Use a separator at the end of the prompt, e.g. \\n\\n###\\n\\n. Remember to also append this separator when you eventually make requests to your model.\n",
    "* Use an ending token at the end of the completion, e.g. END\n",
    "* Remember to add the ending token as a stop sequence during inference, e.g. stop=[\" END\"]\n",
    "* Aim for at least ~500 examples\n",
    "* Ensure that the **prompt + completion doesn't exceed 2048 tokens**, including the separator\n",
    "* Ensure the examples are of high quality and follow the same desired format\n",
    "* Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n",
    "* Using Lower learning rate and only 1-2 epochs tends to work better for these use cases\n",
    "\n",
    "Example:\n",
    "```{\"prompt\":\"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\", \"completion\":\" <engaging ad> END\"}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready!\n",
      "Before training, run in a terminal\n",
      "\t>openai tools fine_tunes.prepare_data -f './dataset/batch/[YOUR_BATCH]/teriyaki.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "batches = [\"500\", \"1000\", \"2000\", \"4000\", \"8000\", \"validation\"]\n",
    "\n",
    "for batch in batches:\n",
    "    dataset = open('./dataset/batches/' + batch + '/teriyaki.jsonl', 'w+')\n",
    "    plans = sorted(os.listdir('./dataset/batches/' + batch + '/plans/'))\n",
    "\n",
    "    # extracting prompts from problems and completions from IPC plans \n",
    "    for i, plan in enumerate(plans):\n",
    "        plan_f = open('./dataset/batches/' + batch + '/plans/' + plan, \"r\")\n",
    "        problem_f = open('./dataset/batches/' + batch + '/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \"r\")\n",
    "        lines = problem_f.readlines()\n",
    "\n",
    "        # lines[49:62] is the range of meaningful predicates at initial state\n",
    "        dataset.write(repr('{\"prompt\":\"\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + \n",
    "              '\\n\\n###\\n\\n\", \"completion\":\" ' + plan_f.read().replace(\"    \", \"\") + ' END\"}')[1:-1] + \"\\n\")\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "print(\"Datasets ready!\")\n",
    "print(\"Before training, run in a terminal\\n\\t>openai tools fine_tunes.prepare_data -f './dataset/batches/[YOUR_BATCH]/teriyaki.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute training cost\n",
    "`1 token ~= 4 chars\n",
    "0.0300$/1K Tokens`\n",
    "\n",
    "In practice it is ~15$ per 1000 samples for this specific planning domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training this dataset will consume about 157412.5 tokens for an estimated cost of 4.72$\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset = open('./dataset/batches/500/teriyaki.jsonl', 'r')\n",
    "chars = len(dataset.read())\n",
    "dataset.close()\n",
    "tokens = chars / 4\n",
    "cost = tokens * 0.00003\n",
    "\n",
    "print(\"Training this dataset will consume about {0} tokens for an estimated cost of {1}$\".format(tokens, round(cost,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a fine-tuned model!\n",
    "\n",
    "### Dataset \n",
    "From the documentation regarding conditional generation:\n",
    "* Aim for at least ~500 examples (8000 recommended)\n",
    "* Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n",
    "* Ensure the examples are of high quality and follow the same desired format\n",
    "* Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n",
    "* Using Lower learning rate and only 1-2 epochs tends to work better for these use cases\n",
    "\n",
    "LEARN MORE ABOUT VALIDATION, DELETING, AND SUFFIX @ \n",
    "\n",
    "https://beta.openai.com/docs/guides/fine-tuning/advanced-usage\n",
    "\n",
    "https://beta.openai.com/docs/api-reference/fine-tunes\n",
    "\n",
    "### Training hyperparameters\n",
    "* `model` use `'davinci'` or `'code-davinci-002'` (CODEX, not yet supported)\n",
    "* `n_epochs` defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. For generative use cases 2 epochs is generally better.\n",
    "* `batch_size` defaults to ~0.2% of the number of examples in the training set, capped at 256. Larger batch sizes tend to work better for larger datasets.\n",
    "* `learning_rate_multiplier`defaults to 0.05, 0.1, or 0.2 depending on final `batch_size`. We recommend experimenting with values in the range 0.02 to 0.2. Larger learning rates often perform better with larger batch sizes.\n",
    "* `compute_classification_metrics`defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.\n",
    "\n",
    "### Results\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:\n",
    "\n",
    "* `elapsed_tokens`: the number of tokens the model has seen so far (including repeats)\n",
    "* `elapsed_examples`: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.\n",
    "* `training_loss`: loss on the training batch\n",
    "* `training_sequence_accuracy`: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n",
    "* `training_token_accuracy`: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-30 23:20:17] Created fine-tune: ft-JNw1L5bGAnUwJdxSXQAQrdbj\n",
      "[2022-11-30 23:20:25] Fine-tune costs $109.78\n",
      "[2022-11-30 23:20:25] Fine-tune enqueued. Queue number: 5\n",
      "[2022-11-30 23:25:17] Fine-tune is in the queue. Queue number: 4\n",
      "[2022-12-01 00:08:39] Fine-tune is in the queue. Queue number: 3\n",
      "[2022-12-01 00:12:43] Fine-tune is in the queue. Queue number: 2\n",
      "[2022-12-01 00:13:08] Fine-tune is in the queue. Queue number: 1\n",
      "[2022-12-01 00:22:56] Fine-tune is in the queue. Queue number: 0\n",
      "[2022-12-01 00:24:34] Fine-tune started\n",
      "[2022-12-01 00:56:47] Completed epoch 1/2\n",
      "[2022-12-01 01:27:47] Completed epoch 2/2\n",
      "[2022-12-01 01:28:34] Uploaded model: davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\n",
      "[2022-12-01 01:28:36] Uploaded result file: file-ce5w8Wzv6QmWtO33b9VY5s6P\n",
      "[2022-12-01 01:28:36] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded 🎉\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# -t jsonl file of the dataset to be used for fine tuning\n",
    "# -v jsonl file of the validation dataset\n",
    "\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.create -t './dataset/batches/500/teriyaki.jsonl' \\\n",
    "-v './dataset/batches/validation/teriyaki.jsonl' \\\n",
    "-m 'davinci' --suffix \"teriyaki-001\" --n_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor results\n",
    "Fine tuning takes time but the client disconnects after a timeout.\n",
    "\n",
    "Use the following code to reconnect and check when it's done. It can be used also for *Further tuning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -i JOB_ID\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.follow -i ft-JNw1L5bGAnUwJdxSXQAQrdbj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further tuning\n",
    "\n",
    "### <ins>Remember to update the -t argument to point to the desired batch jsonl file</ins>\n",
    "Repeat the fine tuning operation for increasingly large batches of samples.\n",
    "\n",
    "### Planning accuracy on the validation set (100/876 samples)\n",
    "\n",
    "| Training set size | Job ID | Model ID | Discarded because out of tokens, >1000 | Accuracy |\n",
    "|-------------------|--------|----------|----------------------------------------|----------|\n",
    "| **500**  | ft-PXtNnUR8jjnAzIgW1YhRpGrt | davinci:ft-teriyaki:teriyaki-001-2022-11-30-18-48-47 | 23 | 2,0%  |\n",
    "| **1000** | ft-EFuAgXbaOpytBR8VGcIq4Qfe | davinci:ft-teriyaki:teriyaki-001-2022-11-30-19-48-12 | 24 | 27,0% |\n",
    "| **2000** | ft-OkqNqBWMm0ATjHB7r2Prsgfj | davinci:ft-teriyaki:teriyaki-001-2022-11-30-21-21-23 |  9 | 59,0% |\n",
    "| **4000** | ft-cldwwOoDt4xGY2WGtTxJo5yh | davinci:ft-teriyaki:teriyaki-001-2022-11-30-22-07-22 |  3 | 84,0% |\n",
    "| **8000** | ft-JNw1L5bGAnUwJdxSXQAQrdbj | davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34 |  1 | 95,0% |\n",
    "|          |                             | **Probe planner**                                    |    | 98,6% |\n",
    "\n",
    "### Notes\n",
    "Not our case, but...\n",
    "\n",
    "```if your new training data is much smaller than your previous training data, you may find it useful to reduce learning_rate_multiplier by a factor of 2 to 4.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|███████████████████████| 646k/646k [00:00<00:00, 307Mit/s]\n",
      "Uploaded file from ./dataset/batches/1000/teriyaki.jsonl: file-WkGWR3hD1sofGiz4lDaFCOJ9\n",
      "Found potentially duplicated files with name 'teriyaki.jsonl', purpose 'fine-tune' and size 1101044 bytes\n",
      "file-xQOM754uLyXXJezwzs1BLlSZ\n",
      "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# -t jsonl file of the dataset to be used for fine tuning\n",
    "# -v jsonl file of the validation dataset\n",
    "\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.create -t './dataset/batches/1000/teriyaki.jsonl' \\\n",
    "-v './dataset/batches/validation/teriyaki.jsonl' \\\n",
    "-m 'davinci:ft-teriyaki:teriyaki-001-2022-11-30-18-48-47' --suffix \"teriyaki-001\" --n_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results\n",
    "We now download the results file for all the fine tunings performed.\n",
    "\n",
    "The `_results.csv` file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:\n",
    "\n",
    "* `elapsed_tokens`: the number of tokens the model has seen so far (including repeats)\n",
    "* `elapsed_examples`: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.\n",
    "* `training_loss`: loss on the training batch\n",
    "* `training_sequence_accuracy`: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n",
    "* `training_token_accuracy`: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83\n",
    "* `validation_loss`: loss on the validation batch\n",
    "* `validation_sequence_accuracy`: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n",
    "* `validation_token_accuracy`: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completion [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-PXtNnUR8jjnAzIgW1YhRpGrt > dataset/batches/500/results_500.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-EFuAgXbaOpytBR8VGcIq4Qfe > dataset/batches/1000/results_1000.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-OkqNqBWMm0ATjHB7r2Prsgfj > dataset/batches/2000/results_2000.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-cldwwOoDt4xGY2WGtTxJo5yh > dataset/batches/4000/results_4000.csv\n",
    "\n",
    "response = !openai -k ***REMOVED*** \\\n",
    "api fine_tunes.results -i ft-JNw1L5bGAnUwJdxSXQAQrdbj > dataset/batches/8000/results_8000.csv\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it!\n",
    "This is an example of a call to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.00100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.00300: (link-to-central-grasp link2 link1 joint3 joint1 gleft gright)\n",
      "0.00500: (decrease_angle_first_child_45 link2 link1 joint1 angle0 angle345 angle330 angle315 gleft gright)\n",
      "0.00700: (decrease_angle_first_child link2 link1 joint1 angle315 angle300 gleft gright)\n",
      "0.00900: (decrease_angle_first_child link2 link1 joint1 angle300 angle285 gleft gright)\n",
      "0.01100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.01300: (link-to-central-grasp link3 link2 joint1 joint2 gleft gright)\n",
      "0.01500: (increase_angle_first_child link3 link2 joint2 angle270 angle285 gleft gright)\n",
      "0.01700: (release-links link3 link2 joint2 gleft gright)\n",
      "0.01900: (link-to-central-grasp link4 link3 joint2 joint3 gleft gright)\n",
      "0.02100: (increase_angle_first_child link4 link3 joint3 angle300 angle315 gleft gright)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\",\n",
    "                                        prompt =  texts,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 600,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "problem = '(:init\\n(angle_joint angle0 joint1)\\n(angle_joint angle345 joint2)\\n(angle_joint angle0 joint3)\\n\\\n",
    "           (in-centre joint3)\\n(in-hand link1)\\n(in-hand link2)\\n(grasp gleft link1)\\n(grasp gright link2))\\n\\\n",
    "           (:goal(and\\n(angle_joint angle285 joint1)\\n(angle_joint angle285 joint2)\\n(angle_joint angle315 joint3)\\\n",
    "           \\n)\\n)\\n)\\n\\n\\n###\\n\\n'\n",
    "\n",
    "GPT_Completion(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teriyaki planning accuracy on the validation set\n",
    "Plan with Teriyaki for all available problems and run VAL Validate\n",
    "\n",
    "Te validation set is composed of 876 samples:\n",
    "* 40 problems were discarded from the original 9000 because they were duplicates\n",
    "* 84 problems were discarded from the original 9000 because Probe failed to solve them and are tested separately\n",
    "* In reality, we usually limit to 100 sample to reduce time and cost\n",
    "\n",
    "### Impact of the call parameters\n",
    "* `temperature` is set to zero to provide deterministic results\n",
    "* `max tokens` for this planning domain 1000 is usually enough but 1200 is safer. Do not increase above 1800 as call+response will exceed the maximum length allowed\n",
    "* `presence_penalty` does not seem to impact results much so I advise to keep it 0. To be confirmed but negative values lead to 1-2% worse performance while positive lead to 1-2% better performance. Might by within the noise error or depend on the domain\n",
    "* `frequency penalty` not tested but supposedly similar to `presence_penalty` in reverse\n",
    "* `best_of` does not seem to have any effect despite being advised for code completion requests\n",
    "\n",
    "### <ins>Remember to update the `engine` argument to point to the desired model</ins>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VAL validation for batch 500\n",
      "Validating --- 11.4% --- Current success rate 2.0%\n",
      "Teriyaki failed 98 plans\n",
      "Starting VAL validation for batch 1000\n",
      "Validating --- 11.4% --- Current success rate 27.0%\n",
      "Teriyaki failed 73 plans\n",
      "Starting VAL validation for batch 2000\n",
      "Validating --- 11.4% --- Current success rate 59.0%\n",
      "Teriyaki failed 41 plans\n",
      "Starting VAL validation for batch 4000\n",
      "Validating --- 11.4% --- Current success rate 84.0%\n",
      "Teriyaki failed 16 plans\n",
      "Starting VAL validation for batch 8000\n",
      "Validating --- 11.4% --- Current success rate 95.0%\n",
      "Teriyaki failed 5 plans\n",
      "Validation complete!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "\n",
    "def GPT_Plan(problem_path, model):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "    problem_f = open(problem_path)\n",
    "    lines = problem_f.readlines()\n",
    "    problem_query = '\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + '\\n\\n###\\n\\n'\n",
    "    problem_f.close()\n",
    "    response = openai.Completion.create(engine=model,\n",
    "                                        prompt =  problem_query,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 1000,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    \n",
    "    if response.choices[0].finish_reason == \"length\":\n",
    "        print(\"WARNING: the query for problem \" + problem_path + \" has exceed the max_tokens limit.\")\n",
    "    return response.choices[0].text\n",
    "\n",
    "snapshots = {\n",
    "    \"500\" : \"davinci:ft-teriyaki:teriyaki-001-2022-11-30-18-48-47\",\n",
    "    \"1000\": \"davinci:ft-teriyaki:teriyaki-001-2022-11-30-19-48-12\",\n",
    "    \"2000\": \"davinci:ft-teriyaki:teriyaki-001-2022-11-30-21-21-23\",\n",
    "    \"4000\": \"davinci:ft-teriyaki:teriyaki-001-2022-11-30-22-07-22\",\n",
    "    \"8000\": \"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\"\n",
    "}\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/batches/validation/problems/'))\n",
    "n_samples = 100 # limits the number of samples to reduce time and cost \n",
    "\n",
    "for batch in snapshots:\n",
    "\n",
    "    # Generating validation dataset of teriyaki plans\n",
    "    # Uncomment on first run!!\n",
    "    \n",
    "#     print(\"Generating {0} validation plans dataset using model trained over {1} samples.\".format(len(problems), batch))\n",
    "    \n",
    "#     for i, problem in enumerate(problems):\n",
    "#         if i == n_samples: ## limit number of iterations\n",
    "#             break\n",
    "#         print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)), end = '\\r')\n",
    "#         problem_path = './dataset/batches/validation/problems/' + problem\n",
    "#         plan = GPT_Plan(problem_path, snapshots[batch])\n",
    "#         plan_f = open('./dataset/batches/validation/results/{0}/plans/{1}'.format(batch, problem)[:-5] + \".plan\", 'w+')\n",
    "#         plan_f.write(plan[1:])\n",
    "#         plan_f.close()\n",
    "#     print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)))\n",
    "        \n",
    "    # Validate with VAL\n",
    "    \n",
    "    print(\"Starting VAL validation for batch {0}\".format(batch))\n",
    "    invalid_plans = 0\n",
    "    success = 0\n",
    "    for i, problem in enumerate(problems):\n",
    "        if i == n_samples: ## limit number of iterations\n",
    "            break\n",
    "        if i > 0:\n",
    "            print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "                round((i / len(problems) * 100), 1),\n",
    "                round((success / i * 100), 1)), end = '\\r')\n",
    "        result_f = open('./dataset/batches/validation/results/' + batch + '/val/' + problem, 'w+')\n",
    "        result = subprocess.run(['./generator/Validate', \n",
    "                                 './dataset/domains/domain_m_25_B.pddl', \n",
    "                                 './dataset/batches/validation/problems/' + problem,\n",
    "                                 './dataset/batches/validation/results/{0}/plans/{1}'.format(batch, problem)[:-5] + \".plan\"], \n",
    "                                 stdout=subprocess.PIPE)\n",
    "        result_f.write(str(result))\n",
    "        result_f.close()\n",
    "        if (\"Plan failed to execute\" in str(result)) or (\"Bad plan description!\" in str(result)):\n",
    "            invalid_plans += 1\n",
    "        else:\n",
    "            success += 1\n",
    "    print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "        round((i / len(problems) * 100), 1),\n",
    "        round((success / i * 100), 1)))\n",
    "\n",
    "    print('Teriyaki failed %i plans' % invalid_plans)\n",
    "    \n",
    "print(\"Validation complete!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teriyaki planning accuracy on the test set (1000 samples)\n",
    "\n",
    "| Training set size | Job ID | Model ID | Discarded because out of tokens, >1200 | Accuracy |\n",
    "|-------------------|--------|----------|----------------------------------------|----------|\n",
    "| **8000** | ft-JNw1L5bGAnUwJdxSXQAQrdbj | davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34 | 8 | 95,5% |\n",
    "|          |                             | **Probe planner**                                    |   | 98,6% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VAL validation on the generated dataset.\n",
      "Validating --- 99.9% --- Current success rate 95.5%\n",
      "Teiyaki failed 46 plans\n",
      "Validation complete!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "\n",
    "# Uncomment only if you need to generate a new test set\n",
    "# %run generator/pddl_confgen.py 4 1000 -r 24 -co\n",
    "# !mv -v generator/problems/conditional/* dataset/test/problems > /dev/null\n",
    "\n",
    "\n",
    "def GPT_Plan(problem_path):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "    problem_f = open(problem_path)\n",
    "    lines = problem_f.readlines()\n",
    "    problem_query = '\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + '\\n\\n###\\n\\n'\n",
    "    problem_f.close()\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\",\n",
    "                                        prompt =  problem_query,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 1200,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    \n",
    "    if response.choices[0].finish_reason == \"length\":\n",
    "        print(\"WARNING: the query for problem \" + problem_path + \" has exceed the max_tokens limit.\")\n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/test/problems/'))\n",
    "\n",
    "# Teriyaki planning\n",
    "\n",
    "print(\"Planning {0} problems with Teriyaki.\".format(len(problems)))\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)), end = '\\r')\n",
    "    problem_path = './dataset/test/problems/' + problem\n",
    "    plan = GPT_Plan(problem_path)\n",
    "    plan_f = open('./dataset/test/plans/{0}'.format(problem)[:-5] + \".plan\", 'w+')\n",
    "    plan_f.write(plan[1:])\n",
    "    plan_f.close()\n",
    "print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)))\n",
    "\n",
    "# Validate with VAL\n",
    "\n",
    "print(\"Starting VAL validation on the generated dataset.\")\n",
    "invalid_plans = 0\n",
    "success = 0\n",
    "for i, problem in enumerate(problems):\n",
    "    if i > 0:\n",
    "        print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "            round((i / len(problems) * 100), 1),\n",
    "            round((success / i * 100), 1)), end = '\\r')\n",
    "    result_f = open('./dataset/test/val/' + problem, 'w+')\n",
    "    result = subprocess.run(['./generator/Validate', \n",
    "                             './dataset/domains/domain_m_25_B.pddl', \n",
    "                             './dataset/test/problems/' + problem,\n",
    "                             './dataset/test/plans/{0}'.format(problem)[:-5] + \".plan\"], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    result_f.write(str(result))\n",
    "    result_f.close()\n",
    "    if (\"Plan failed to execute\" in str(result)) or (\"Bad plan description!\" in str(result)):\n",
    "        invalid_plans += 1\n",
    "    else:\n",
    "        success += 1\n",
    "print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "    round((i / len(problems) * 100), 1),\n",
    "    round((success / i * 100), 1)))\n",
    "\n",
    "print('Teiyaki failed %i plans' % invalid_plans)\n",
    "\n",
    "print(\"Validation complete!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Teriyaki success rate on problems failed by probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VAL validation on the generated dataset.\n",
      "Validating --- 98.8% --- Current success rate 98.8%%\n",
      "Teriyaki failed 2 plans\n",
      "Validation complete!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "\n",
    "def GPT_Plan(problem_path):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "    problem_f = open(problem_path)\n",
    "    lines = problem_f.readlines()\n",
    "    problem_query = '\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + '\\n\\n###\\n\\n'\n",
    "    problem_f.close()\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\",\n",
    "                                        prompt =  problem_query,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 1200,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    \n",
    "    if response.choices[0].finish_reason == \"length\":\n",
    "        print(\"WARNING: the query for problem \" + problem_path + \" has exceed the max_tokens limit.\")\n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/batches/validation/unsolved/problems/'))\n",
    "\n",
    "# Planning with Teriyaki\n",
    "\n",
    "print(\"Generating {0} validation plans dataset using model trained over 8000 samples.\".format(len(problems)))\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)), end = '\\r')\n",
    "    problem_path = './dataset/batches/validation/unsolved/problems/' + problem\n",
    "    plan = GPT_Plan(problem_path)\n",
    "    plan_f = open('./dataset/batches/validation/unsolved/plans/{0}'.format(problem)[:-5] + \".plan\", 'w+')\n",
    "    plan_f.write(plan[1:])\n",
    "    plan_f.close()\n",
    "print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)))\n",
    "\n",
    "# Validate with VAL\n",
    "\n",
    "print(\"Starting VAL validation on the generated dataset.\")\n",
    "invalid_plans = 0\n",
    "success = 0\n",
    "for i, problem in enumerate(problems):\n",
    "    if i > 0:\n",
    "        print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "            round((i / len(problems) * 100), 1),\n",
    "            round((success / i * 100), 1)), end = '\\r')\n",
    "    result_f = open('./dataset/batches/validation/unsolved/val/' + problem, 'w+')\n",
    "    result = subprocess.run(['./generator/Validate', \n",
    "                             './dataset/domains/domain_m_25_B.pddl', \n",
    "                             './dataset/batches/validation/unsolved/problems/' + problem,\n",
    "                             './dataset/batches/validation/unsolved/plans/{0}'.format(problem)[:-5] + \".plan\"], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    result_f.write(str(result))\n",
    "    result_f.close()\n",
    "    if (\"Plan failed to execute\" in str(result)) or (\"Bad plan description!\" in str(result)):\n",
    "        invalid_plans += 1\n",
    "    else:\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "    round((i / len(problems) * 100), 1),\n",
    "    round((success / i * 100), 1)))\n",
    "\n",
    "print('Teriyaki failed %i plans' % invalid_plans)\n",
    "    \n",
    "print(\"Validation complete!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare planning times and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "def GPT_Plan(problem_path):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "    problem_f = open(problem_path)\n",
    "    lines = problem_f.readlines()\n",
    "    problem_query = '\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + '\\n\\n###\\n\\n'\n",
    "    problem_f.close()\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\",\n",
    "                                        prompt =  problem_query,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 1000,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    \n",
    "    if response.choices[0].finish_reason == \"length\":\n",
    "        print(\"WARNING: the query for problem \" + problem_path + \" has exceed the max_tokens limit.\")\n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/batches/validation/problems/'))\n",
    "print(\"{0} problems loaded\".format(len(problems)))\n",
    "\n",
    "results = np.empty([2,6])\n",
    "timings = np.empty([876,2])\n",
    "\n",
    "### Test Probe\n",
    "\n",
    "comparison_results = []\n",
    "planning_times = []\n",
    "planning_steps = []\n",
    "fails = 0\n",
    "\n",
    "print(\"Running Probe tests --- 0%\", end = '\\r')\n",
    "for j, problem in enumerate(problems):\n",
    "    result = subprocess.run(['./generator/probe', \n",
    "                             '-d', './dataset/domains/domain_m_25_B.pddl', \n",
    "                             '-i', './dataset/problems/' + problem,\n",
    "                             '-o', './dataset/plans/m_25_B-' + str(problem.split('_',2)[2:][0])], \n",
    "                             stdout=subprocess.PIPE)\n",
    "\n",
    "    # considering failures plans that took more than 1sec or that failed planning\n",
    "    try:\n",
    "        planning_steps.append(float(result.stdout.decode(\"utf8\").split('steps: ')[1].split('\\n')[0]))\n",
    "        ptime = float(result.stdout.decode(\"utf8\").split('Time: ')[1].split('\\n')[0])\n",
    "        planning_times.append(ptime)\n",
    "        if ptime > 2: fails+=1\n",
    "\n",
    "    except IndexError:\n",
    "        planning_times.append(np.nan) \n",
    "        fails+=1\n",
    "\n",
    "    print(\"Running Probe tests --- \" + str(round((j / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print(\"Running Probe tests --- 100%\")\n",
    "\n",
    "timings[:,0] = planning_times\n",
    "\n",
    "planning_times = np.array(planning_times)\n",
    "planning_steps = np.array(planning_steps)\n",
    "\n",
    "comparison_results.append(np.average(planning_steps))\n",
    "comparison_results.append(np.amin(planning_times))\n",
    "comparison_results.append(np.amax(planning_times))\n",
    "comparison_results.append(np.average(planning_times))\n",
    "comparison_results.append(np.std(planning_times))\n",
    "\n",
    "comparison_results.append(fails)\n",
    "\n",
    "results[0] = comparison_results\n",
    "\n",
    "print(results)\n",
    "\n",
    "### Test Teriyaki\n",
    "\n",
    "comparison_results = []\n",
    "planning_times = []\n",
    "planning_steps = []\n",
    "fails = 0\n",
    "\n",
    "print(\"Running Teriyaki tests --- 0%\", end = '\\r')\n",
    "for j, problem in enumerate(problems):\n",
    "    st = time.time()\n",
    "    result = GPT_Plan('./dataset/batches/validation/problems/' + problem)\n",
    "    et = time.time()\n",
    "    # considering failures plans that took more than 1sec or that failed planning\n",
    "    try:\n",
    "        planning_steps.append(float(result.count('\\n')))\n",
    "        planning_times.append(et - st)\n",
    "        if (et - st) > 2: fails+=1\n",
    "\n",
    "    except IndexError:\n",
    "        planning_times.append(np.nan) \n",
    "        fails+=1\n",
    "\n",
    "    print(\"Running Teriyaki tests --- \" + str(round((j / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print(\"Running Teriyaki tests --- 100%\")\n",
    "\n",
    "timings[:,1] = planning_times\n",
    "\n",
    "planning_times = np.array(planning_times)\n",
    "planning_steps = np.array(planning_steps)\n",
    "\n",
    "comparison_results.append(np.average(planning_steps))\n",
    "comparison_results.append(np.amin(planning_times))\n",
    "comparison_results.append(np.amax(planning_times))\n",
    "comparison_results.append(np.average(planning_times))\n",
    "comparison_results.append(np.std(planning_times))\n",
    "\n",
    "comparison_results.append(fails)\n",
    "\n",
    "results[1] = comparison_results\n",
    "    \n",
    "print(\"Tests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2acb6_row1_col1, #T_2acb6_row1_col5 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2acb6\" style=\"font-size: 20px\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2acb6_level0_col0\" class=\"col_heading level0 col0\" >SOLVER</th>\n",
       "      <th id=\"T_2acb6_level0_col1\" class=\"col_heading level0 col1\" >avg_steps</th>\n",
       "      <th id=\"T_2acb6_level0_col2\" class=\"col_heading level0 col2\" >min</th>\n",
       "      <th id=\"T_2acb6_level0_col3\" class=\"col_heading level0 col3\" >max</th>\n",
       "      <th id=\"T_2acb6_level0_col4\" class=\"col_heading level0 col4\" >avg</th>\n",
       "      <th id=\"T_2acb6_level0_col5\" class=\"col_heading level0 col5\" >std</th>\n",
       "      <th id=\"T_2acb6_level0_col6\" class=\"col_heading level0 col6\" >fails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2acb6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2acb6_row0_col0\" class=\"data row0 col0\" >Probe</td>\n",
       "      <td id=\"T_2acb6_row0_col1\" class=\"data row0 col1\" >11.225</td>\n",
       "      <td id=\"T_2acb6_row0_col2\" class=\"data row0 col2\" >0.013</td>\n",
       "      <td id=\"T_2acb6_row0_col3\" class=\"data row0 col3\" >24.033</td>\n",
       "      <td id=\"T_2acb6_row0_col4\" class=\"data row0 col4\" >2.409</td>\n",
       "      <td id=\"T_2acb6_row0_col5\" class=\"data row0 col5\" >3.713</td>\n",
       "      <td id=\"T_2acb6_row0_col6\" class=\"data row0 col6\" >223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2acb6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2acb6_row1_col0\" class=\"data row1 col0\" >Teiryaki</td>\n",
       "      <td id=\"T_2acb6_row1_col1\" class=\"data row1 col1\" >10.394</td>\n",
       "      <td id=\"T_2acb6_row1_col2\" class=\"data row1 col2\" >1.112</td>\n",
       "      <td id=\"T_2acb6_row1_col3\" class=\"data row1 col3\" >30.650</td>\n",
       "      <td id=\"T_2acb6_row1_col4\" class=\"data row1 col4\" >9.311</td>\n",
       "      <td id=\"T_2acb6_row1_col5\" class=\"data row1 col5\" >3.619</td>\n",
       "      <td id=\"T_2acb6_row1_col6\" class=\"data row1 col6\" >875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f80973316a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    is_min[0] = False\n",
    "    return ['background-color: yellow' if v else '' for v in is_min]\n",
    "\n",
    "results_table = pd.DataFrame(results, columns=('avg_steps', 'min', 'max', 'avg', 'std', 'fails'))\n",
    "\n",
    "df = pd.DataFrame({'SOLVER': ('Probe', 'Teiryaki')})\n",
    "df = pd.concat([df, results_table],axis=1)\n",
    "df = df.astype({\"fails\":\"int\"})\n",
    "\n",
    "df.style.\\\n",
    "    apply(highlight_min).\\\n",
    "    format(precision=3).\\\n",
    "    set_table_attributes('style=\"font-size: 20px\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dft = pd.DataFrame(timings, columns=['Probe','Teriyaki'])\n",
    "\n",
    "props = dict(boxes=\"DarkBlue\", \n",
    "             whiskers=\"DarkBlue\", \n",
    "             medians=\"Yellow\", \n",
    "             caps=\"Gray\")\n",
    "\n",
    "boxplot = dft.plot.box(figsize=(16,9), \n",
    "                       fontsize=20, \n",
    "                       color=props, \n",
    "                       patch_artist=True,\n",
    "                       xlabel=\"Solver\",\n",
    "                       ylabel=\"seconds\",\n",
    "                       showfliers=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did we impact the natural language capabilities of GPT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Davinci:\n",
      "\n",
      "\n",
      "\n",
      "A large language model is a statistical model that is trained on a large corpus of text data in order to predict the probability of a sequence of words. The model is typically a recurrent neural network (RNN) or a long short-term memory (LSTM) network.\n",
      "\n",
      "===========\n",
      "Teriyaki:\n",
      "\n",
      "\n",
      "\n",
      "A large language model is a model that is sufficiently large to contain a sufficiently large number of words.\n",
      "\n",
      "If you want to build a good language model, you need a sufficiently large language model.\n",
      "\n",
      "The size of a language model depends on the following:\n",
      "\n",
      "The amount of data (and the size of the data)\n",
      "\n",
      "The size of the vocabulary\n",
      "\n",
      "The size of the training set\n",
      "\n",
      "The size of the test set\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the vocab\n",
      "\n",
      "The size of the training set\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the test set\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the training set\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the training set\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the training set\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the training set\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size of the input\n",
      "\n",
      "The size of the output\n",
      "\n",
      "The size\n",
      "\n",
      "Testing queries that mix natural language and our planning model\n",
      "\n",
      "\n",
      "===========\n",
      "GPT Davinci mixed query:\n",
      "\n",
      "The last action that would be taken is to release the grasp on link2.\n",
      "\n",
      "===========\n",
      "Teriyaki Mixed query:\n",
      " 0.00100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.00300: (link-to-central-grasp link2 link1 joint3 joint1 gleft gright)\n",
      "0.00500: (decrease_angle_first_child_45 link2 link1 joint1 angle0 angle345 angle330 angle315 gleft gright)\n",
      "0.00700: (decrease_angle_first_child link2 link1 joint1 angle315 angle300 gleft gright)\n",
      "0.00900: (decrease_angle_first_child link2 link1 joint1 angle300 angle285 gleft gright)\n",
      "0.01100: (release-links link2 link1 joint1 gleft gright)\n",
      "0.01300: (link-to-central-grasp link3 link2 joint1 joint2 gleft gright)\n",
      "0.01500: (increase_angle_first_child link3 link2 joint2 angle270 angle285 gleft gright)\n",
      "0.01700: (release-links link3 link2 joint2 gleft gright)\n",
      "0.01900: (link-to-central-grasp link4 link3 joint2 joint3 gleft gright)\n",
      "0.02100: (increase_angle_first_child link4 link3 joint3 angle300 angle315 gleft gright)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def GPT_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"text-davinci-002\",\n",
    "                                        prompt =  texts,\n",
    "                                        temperature = 0.6,\n",
    "                                        top_p = 1,\n",
    "                                        max_tokens = 600,\n",
    "                                        frequency_penalty = 0,\n",
    "                                        presence_penalty = 0,\n",
    "                                        stop = \"END\")\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "def Teriyaki_Completion(texts):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\",\n",
    "                                        prompt =  texts,\n",
    "                                        temperature = 0.6,\n",
    "                                        top_p = 1,\n",
    "                                        max_tokens = 600,\n",
    "                                        frequency_penalty = 0,\n",
    "                                        presence_penalty = 0,\n",
    "                                        stop = \"END\")\n",
    "    return print(response.choices[0].text)\n",
    "\n",
    "query = \"What is a Large Language Model?\"\n",
    "\n",
    "print(\"GPT Davinci:\\n\")\n",
    "GPT_Completion(query)\n",
    "\n",
    "print(\"\\n===========\\nTeriyaki:\\n\")\n",
    "Teriyaki_Completion(query)\n",
    "\n",
    "print(\"\\nTesting queries that mix natural language and our planning model\\n\")\n",
    "\n",
    "mixed_query = \"If you solve this problem, what would be the last action taken?\\n\\\n",
    "               Problem:\\n\\\n",
    "               (:init\\n(angle_joint angle0 joint1)\\n(angle_joint angle345 joint2)\\n(angle_joint angle0 joint3)\\n\\\n",
    "               (in-centre joint3)\\n(in-hand link1)\\n(in-hand link2)\\n(grasp gleft link1)\\n(grasp gright link2))\\n\\\n",
    "               (:goal(and\\n(angle_joint angle285 joint1)\\n(angle_joint angle285 joint2)\\n(angle_joint angle315 joint3\\\n",
    "               )\\n)\\n)\\n)\\n\\n\\n###\\n\\n\" ## The prompt ending we used in training should suggest to recall that \n",
    "\n",
    "print(\"\\n===========\\nGPT Davinci mixed query:\")\n",
    "GPT_Completion(mixed_query)\n",
    "\n",
    "print(\"\\n===========\\nTeriyaki Mixed query:\")\n",
    "Teriyaki_Completion(mixed_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - No macro\n",
    "\n",
    "### Learning from scratch\n",
    "\n",
    "| Training set size | Job ID | Model ID | Accuracy |\n",
    "|-------------------|---------------------------|------------------------------------------------------|----------|\n",
    "| No macro - 1000   |ft-3fwBVR15jYUsva3pUC6OsTb4 | davinci:ft-teriyaki:teriyaki-003-2023-02-08-16-04-36 | |\n",
    "\n",
    "### Learning from Teriyaki-001\n",
    "\n",
    "| Training set size | Job ID | Model ID | Accuracy |\n",
    "|-------------------|---------------------------|------------------------------------------------------|----------|\n",
    "| No macro - 1000 | ft-fnYjeF0c5JHLVivXYtcURsQG | davinci:ft-teriyaki:teriyaki-002-2023-02-07-13-55-29 | 2.4% |\n",
    "| No macro - 2000 | ft-ZdKuOAYYbedWMPvw0Pm2EV8c | davinci:ft-teriyaki:teriyaki-002-2023-02-07-19-44-07 | |\n",
    "\n",
    "Run as many time as needed, but remember to update the base model in the call at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conditional problem files to /home/alessio/planning/teriyaki/generator/problems/conditional...\n",
      "Done.\n",
      "All operations completed. Happy planning!\n",
      "Planning --- 35.6%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::bad_alloc'\n",
      "  what():  std::bad_alloc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plans converted to IPC format\n",
      "Datasets ready!\n",
      "Before training, run in a terminal\n",
      "\t>openai tools fine_tunes.prepare_data -f './dataset/batches/[YOUR_BATCH]/teriyaki.jsonl'\n",
      "Upload progress: 100%|████████████████████| 2.11M/2.11M [00:00<00:00, 2.85Git/s]\n",
      "Uploaded file from ./dataset/batches/no_macro/teriyaki_no_macro.jsonl: file-QOcI8DCY5VZmg26iDHdMD5gQ\n",
      "Found potentially duplicated files with name 'teriyaki_no_macro.jsonl', purpose 'fine-tune' and size 194536 bytes\n",
      "file-DMReY0ylJ69c7yJDg8TNhXUm\n",
      "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -r dataset/batches/no_macro &> /dev/null\n",
    "!rm -r dataset/batches/validation/no_macro &> /dev/null\n",
    "!rm -r dataset/batches/validation/results/no_macro\n",
    "!mkdir dataset/batches/no_macro/\n",
    "!mkdir dataset/batches/no_macro/problems\n",
    "!mkdir dataset/batches/no_macro/plans\n",
    "!mkdir dataset/batches/no_macro/plans_ipc\n",
    "!mkdir dataset/batches/validation/results/no_macro\n",
    "!mkdir dataset/batches/validation/results/no_macro/plans\n",
    "!mkdir dataset/batches/validation/results/no_macro/val\n",
    "!mkdir dataset/batches/validation/no_macro\n",
    "!mkdir dataset/batches/validation/no_macro/plans\n",
    "!mkdir dataset/batches/validation/no_macro/problems\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "## generate problems\n",
    "\n",
    "%run generator/pddl_confgen.py 4 1000 -r 24 -co\n",
    "!mv -v generator/problems/conditional/* dataset/batches/no_macro/problems > /dev/null\n",
    "\n",
    "## generate plans\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/batches/no_macro/problems'))\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    \n",
    "    result = subprocess.run(['./generator/probe', \n",
    "                             '-d', './dataset/domains/domain_m_0.pddl', \n",
    "                             '-i', './dataset/batches/no_macro/problems/' + problem,\n",
    "                             '-o', './dataset/batches/no_macro/plans/m_0-' + str(problem.split('_',2)[2:][0])], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    print(\"Planning --- \" + str(round((i / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "## convert to IPC    \n",
    "    \n",
    "plans = sorted(os.listdir('./dataset/batches/no_macro/plans/'))\n",
    "\n",
    "for plan in plans:\n",
    "    probe = open('./dataset/batches/no_macro/plans/' + plan, \"r\")\n",
    "    val = open('./dataset/batches/no_macro/plans_ipc/' + plan.split(\".\")[0] + \".plan\", 'w+')\n",
    "    replacement = \"\"\n",
    "    # using the for loop\n",
    "    action_timing = 1\n",
    "    for line in probe:\n",
    "        line = line.lower()\n",
    "        line = '%.3f' % (action_timing / 1000) + \"00: \" + line\n",
    "        val.write(line)\n",
    "        action_timing += 2\n",
    "\n",
    "    probe.close()\n",
    "    val.close()\n",
    "    \n",
    "print(\"Plans converted to IPC format\")\n",
    "\n",
    "## sort plans\n",
    "## Uncomment only if you need to generate a validation dataset\n",
    "\n",
    "plans_ipc = sorted(os.listdir('./dataset/batches/no_macro/plans_ipc/'))\n",
    "\n",
    "for i, plan in enumerate(plans_ipc):\n",
    "    if i < 1000:\n",
    "        continue\n",
    "    else:\n",
    "        shutil.move('./dataset/batches/no_macro/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \n",
    "                     'dataset/batches/validation/no_macro/problems/')\n",
    "        shutil.move('./dataset/batches/no_macro/plans_ipc/' + plans_ipc[i], 'dataset/batches/validation/no_macro/plans/')\n",
    "\n",
    "print('All problems and plans have been sorted into batches.')\n",
    "\n",
    "# compile datasets\n",
    "\n",
    "dataset = open('./dataset/batches/no_macro/teriyaki_no_macro.jsonl', 'w+')\n",
    "plans_ipc = sorted(os.listdir('./dataset/batches/no_macro/plans_ipc/'))\n",
    "\n",
    "for i, plan in enumerate(plans_ipc):\n",
    "    plan_f = open('./dataset/batches/no_macro/plans_ipc/' + plan, \"r\")\n",
    "    problem_f = open('./dataset/batches/no_macro/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \"r\")\n",
    "    lines = problem_f.readlines()\n",
    "\n",
    "    dataset.write(repr('{\"prompt\":\"\\n--NO-MACRO\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + \n",
    "          '\\n\\n###\\n\\n\", \"completion\":\" ' + plan_f.read().replace(\"    \", \"\") + ' END\"}')[1:-1] + \"\\n\")\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "dataset = open('./dataset/batches/validation/no_macro/teriyaki_no_macro.jsonl', 'w+')\n",
    "plans_ipc = sorted(os.listdir('./dataset/batches/validation/no_macro/plans/'))\n",
    "\n",
    "for i, plan in enumerate(plans_ipc):\n",
    "    plan_f = open('./dataset/batches/validation/no_macro/plans/' + plan, \"r\")\n",
    "    problem_f = open('./dataset/batches/validation/no_macro/problems/problem_conditional_' + plan.split(\"-\")[1].split(\".\")[0] + \".pddl\", \"r\")\n",
    "    lines = problem_f.readlines()\n",
    "\n",
    "    dataset.write(repr('{\"prompt\":\"\\n--NO-MACRO\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + \n",
    "          '\\n\\n###\\n\\n\", \"completion\":\" ' + plan_f.read().replace(\"    \", \"\") + ' END\"}')[1:-1] + \"\\n\")\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "print(\"Datasets ready!\")\n",
    "print(\"Before training, run in a terminal\\n\\t>openai tools fine_tunes.prepare_data -f './dataset/batches/[YOUR_BATCH]/teriyaki.jsonl'\")\n",
    "\n",
    "# fine-tune starting from previous model\n",
    "\n",
    "!openai -k ***REMOVED*** \\\n",
    "api fine_tunes.create -t './dataset/batches/no_macro/teriyaki_no_macro.jsonl' \\\n",
    "-v './dataset/batches/validation/no_macro/teriyaki_no_macro.jsonl' \\\n",
    "-m 'davinci' --suffix \"teriyaki-003\" --n_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Teriyaki no macro success rate\n",
    "\n",
    "At 1000 samples:\n",
    "```Validating --- 98.8% --- Current success rate 2.4%\n",
    "82 plans could not be validated and have been removed from the dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VAL validation on the generated dataset.\n",
      "./dataset/batches/validation/no_macro/problems/problem_conditional_4_24_01017.pddl\n",
      "./dataset/batches/validation/results/no_macro/plans/problem_conditional_4_24_01017.plan\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         success \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating --- \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m --- Current success rate \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mround\u001b[39m((i \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(problems) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m), \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28mround\u001b[39m((\u001b[43msuccess\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m), \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeriyaki-no-macro failed to plan \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m problems\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m invalid_plans)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation complete!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# import subprocess\n",
    "\n",
    "# def GPT_Plan(problem_path):## Call the API key under your account (in a secure way)\n",
    "#     openai.api_key = \"***REMOVED***\"\n",
    "#     problem_f = open(problem_path)\n",
    "#     lines = problem_f.readlines()\n",
    "#     problem_query = '\\n--NO-MACRO\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + '\\n\\n###\\n\\n'\n",
    "#     problem_f.close()\n",
    "#     response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-003-2023-02-08-16-04-36\",\n",
    "#                                         prompt =  problem_query,\n",
    "#                                         top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "#                                         temperature = 0, # argmax\n",
    "#                                         max_tokens = 1800, # increased to accomodate for longer plans\n",
    "#                                         presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "#                                         frequency_penalty = 0,\n",
    "#                                         # best_of = 4, # 3-5 work well for code completion\n",
    "#                                         stop = \"END\")\n",
    "    \n",
    "#     if response.choices[0].finish_reason == \"length\":\n",
    "#         print(\"WARNING: the query for problem \" + problem_path + \" has exceed the max_tokens limit.\")\n",
    "#     return response.choices[0].text\n",
    "\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/batches/validation/no_macro/problems/'))\n",
    "\n",
    "# # Generating validation dataset of plans\n",
    "\n",
    "# print(\"Generating {0} validation plans dataset using no macro model.\".format(len(problems)))\n",
    "\n",
    "# for i, problem in enumerate(problems):\n",
    "#     print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)), end = '\\r')\n",
    "#     problem_path = './dataset/batches/validation/no_macro/problems/' + problem\n",
    "#     plan = GPT_Plan(problem_path)\n",
    "#     plan_f = open('./dataset/batches/validation/results/no_macro/plans/{0}'.format(problem)[:-5] + \".plan\", 'w+')\n",
    "#     plan_f.write(plan[1:])\n",
    "#     plan_f.close()\n",
    "# print(\"GPT-Planning --- {0}%\".format(round((i / len(problems) * 100), 1)))\n",
    "\n",
    "# Validate with VAL\n",
    "\n",
    "print(\"Starting VAL validation on the generated dataset.\")\n",
    "invalid_plans = 0\n",
    "success = 0\n",
    "for i, problem in enumerate(problems):\n",
    "    if i > 0:\n",
    "        print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "            round((i / len(problems) * 100), 1),\n",
    "            round((success / i * 100), 1)), end = '\\r')\n",
    "    result_f = open('./dataset/batches/validation/results/no_macro/val/' + problem, 'w+')\n",
    "    print('./dataset/batches/validation/no_macro/problems/' + problem)\n",
    "    print('./dataset/batches/validation/results/no_macro/plans/{0}'.format(problem)[:-5] + \".plan\")\n",
    "    result = subprocess.run(['./generator/Validate', \n",
    "                             './dataset/domains/domain_m_0.pddl', \n",
    "                             './dataset/batches/validation/no_macro/problems/' + problem,\n",
    "                             './dataset/batches/validation/results/no_macro/plans/{0}'.format(problem)[:-5] + \".plan\"], \n",
    "                             stdout=subprocess.PIPE)\n",
    "    result_f.write(str(result))\n",
    "    result_f.close()\n",
    "    if (\"Plan failed to execute\" in str(result)) or (\"Bad plan description!\" in str(result)):\n",
    "        invalid_plans += 1\n",
    "    else:\n",
    "        print(problem)\n",
    "        success += 1\n",
    "    break\n",
    "print(\"Validating --- {0}% --- Current success rate {1}%\".format(\n",
    "    round((i / len(problems) * 100), 1),\n",
    "    round((success / i * 100), 1)))\n",
    "\n",
    "print('Teriyaki-no-macro failed to plan %i problems' % invalid_plans)\n",
    "    \n",
    "print(\"Validation complete!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare no macro planning times and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "def GPT_Plan(problem_path):## Call the API key under your account (in a secure way)\n",
    "    openai.api_key = \"***REMOVED***\"\n",
    "    problem_f = open(problem_path)\n",
    "    lines = problem_f.readlines()\n",
    "    problem_query = '\\n--NO-MACRO\\n(:init\\n' + ''.join(lines[49:-1]).replace(\"    \", \"\") + '\\n\\n###\\n\\n'\n",
    "    problem_f.close()\n",
    "    response = openai.Completion.create(engine=\"davinci:ft-teriyaki:teriyaki-001-2022-12-01-00-28-34\",\n",
    "                                        prompt =  problem_query,\n",
    "                                        top_p = 1,       # modify ONLY top_p or temperature, not both       \n",
    "                                        temperature = 0, # argmax\n",
    "                                        max_tokens = 1900,\n",
    "                                        presence_penalty = 0, # -2 < x < 2, negative favors repetitions\n",
    "                                        frequency_penalty = 0,\n",
    "                                        # best_of = 4, # 3-5 work well for code completion\n",
    "                                        stop = \"END\")\n",
    "    \n",
    "    if response.choices[0].finish_reason == \"length\":\n",
    "        print(\"WARNING: the query for problem \" + problem_path + \" has exceed the max_tokens limit.\")\n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "problems = sorted(os.listdir('./dataset/batches/validation/no_macro/problems/'))\n",
    "print(\"{0} problems loaded\".format(len(problems)))\n",
    "\n",
    "results = np.empty([2,6])\n",
    "timings = np.empty([84,2])\n",
    "\n",
    "### Test Probe\n",
    "\n",
    "comparison_results = []\n",
    "planning_times = []\n",
    "planning_steps = []\n",
    "fails = 0\n",
    "\n",
    "print(\"Running Probe tests --- 0%\", end = '\\r')\n",
    "for j, problem in enumerate(problems):\n",
    "    result = subprocess.run(['./generator/probe', \n",
    "                             '-d', './dataset/domains/domain_m_0.pddl', \n",
    "                             '-i', './dataset/batches/validation/no_macro/problems/' + problem,\n",
    "                             '-o', './dataset/batches/no_macro/plans/m_0-' + str(problem.split('_',2)[2:][0])], \n",
    "                             stdout=subprocess.PIPE)\n",
    "\n",
    "    # considering failures plans that took more than 1sec or that failed planning\n",
    "    try:\n",
    "        planning_steps.append(float(result.stdout.decode(\"utf8\").split('steps: ')[1].split('\\n')[0]))\n",
    "        ptime = float(result.stdout.decode(\"utf8\").split('Time: ')[1].split('\\n')[0])\n",
    "        planning_times.append(ptime)\n",
    "        if ptime > 2: fails+=1\n",
    "\n",
    "    except IndexError:\n",
    "        planning_times.append(np.nan) \n",
    "        fails+=1\n",
    "\n",
    "    print(\"Running Probe tests --- \" + str(round((j / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print(\"Running Probe tests --- 100%\")\n",
    "\n",
    "timings[:,0] = planning_times\n",
    "\n",
    "planning_times = np.array(planning_times)\n",
    "planning_steps = np.array(planning_steps)\n",
    "\n",
    "comparison_results.append(np.average(planning_steps))\n",
    "comparison_results.append(np.amin(planning_times))\n",
    "comparison_results.append(np.amax(planning_times))\n",
    "comparison_results.append(np.average(planning_times))\n",
    "comparison_results.append(np.std(planning_times))\n",
    "\n",
    "comparison_results.append(fails)\n",
    "\n",
    "results[0] = comparison_results\n",
    "\n",
    "print(results)\n",
    "\n",
    "### Test Teriyaki\n",
    "\n",
    "comparison_results = []\n",
    "planning_times = []\n",
    "planning_steps = []\n",
    "fails = 0\n",
    "\n",
    "print(\"Running Teriyaki tests --- 0%\", end = '\\r')\n",
    "for j, problem in enumerate(problems):\n",
    "    st = time.time()\n",
    "    result = GPT_Plan('./dataset/batches/validation/no_macro/problems/' + problem)\n",
    "    et = time.time()\n",
    "    # considering failures plans that took more than 1sec or that failed planning\n",
    "    try:\n",
    "        planning_steps.append(float(result.count('\\n')))\n",
    "        planning_times.append(et - st)\n",
    "        if (et - st) > 2: fails+=1\n",
    "\n",
    "    except IndexError:\n",
    "        planning_times.append(np.nan) \n",
    "        fails+=1\n",
    "\n",
    "    print(\"Running Teriyaki tests --- \" + str(round((j / len(problems) * 100), 1)) +\"%\", end = '\\r')\n",
    "\n",
    "print(\"Running Teriyaki tests --- 100%\")\n",
    "\n",
    "timings[:,1] = planning_times\n",
    "\n",
    "planning_times = np.array(planning_times)\n",
    "planning_steps = np.array(planning_steps)\n",
    "\n",
    "comparison_results.append(np.average(planning_steps))\n",
    "comparison_results.append(np.amin(planning_times))\n",
    "comparison_results.append(np.amax(planning_times))\n",
    "comparison_results.append(np.average(planning_times))\n",
    "comparison_results.append(np.std(planning_times))\n",
    "\n",
    "comparison_results.append(fails)\n",
    "\n",
    "results[1] = comparison_results\n",
    "    \n",
    "print(\"Tests completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    is_min[0] = False\n",
    "    return ['background-color: yellow' if v else '' for v in is_min]\n",
    "\n",
    "results_table = pd.DataFrame(results, columns=('avg_steps', 'min', 'max', 'avg', 'std', 'fails'))\n",
    "\n",
    "df = pd.DataFrame({'SOLVER': ('Probe', 'Teiryaki')})\n",
    "df = pd.concat([df, results_table],axis=1)\n",
    "df = df.astype({\"fails\":\"int\"})\n",
    "\n",
    "df.style.\\\n",
    "    apply(highlight_min).\\\n",
    "    format(precision=3).\\\n",
    "    set_table_attributes('style=\"font-size: 20px\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(timings, columns=['Probe','Teriyaki'])\n",
    "\n",
    "props = dict(boxes=\"DarkBlue\", \n",
    "             whiskers=\"DarkBlue\", \n",
    "             medians=\"Yellow\", \n",
    "             caps=\"Gray\")\n",
    "\n",
    "boxplot = dft.plot.box(figsize=(16,9), \n",
    "                       fontsize=20, \n",
    "                       color=props, \n",
    "                       patch_artist=True,\n",
    "                       xlabel=\"Solver\",\n",
    "                       ylabel=\"seconds\",\n",
    "                       showfliers=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
